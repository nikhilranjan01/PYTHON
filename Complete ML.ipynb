{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
    "## **Experiment 5: KNN Algorithm**\n",
    "  \n",
    "\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split , KFold\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from collections import Counter\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "iris_df = pd.DataFrame(data= np.c_[iris['data'], iris['target']],\n",
    "                      columns= iris['feature_names'] + ['target'])\n",
    "iris_df.head()\n",
    "iris_df.describe()\n",
    "\n",
    "x= iris_df.iloc[:, :-1]\n",
    "y= iris_df.iloc[:, -1]\n",
    "\n",
    "x_train, x_test, y_train, y_test= train_test_split(x, y,test_size= 0.2,shuffle= True,random_state= 0)\n",
    "x_train= np.array(x_train)\n",
    "y_train= np.array(y_train)\n",
    "\n",
    "x_test= np.array(x_test)\n",
    "y_test= np.array(y_test)\n",
    "\n",
    "scaler= Normalizer().fit(x_train) # the scaler is fitted to the training set\n",
    "normalized_x_train= scaler.transform(x_train) # the scaler is applied to the training set\n",
    "normalized_x_test= scaler.transform(x_test) # the scaler is applied to the test set\n",
    "\n",
    "di= {0.0: 'Setosa', 1.0: 'Versicolor', 2.0:'Virginica'} # dictionary\n",
    "\n",
    "before= sns.pairplot(iris_df.replace({'target': di}), hue= 'target')\n",
    "before.fig.suptitle('Pair Plot of the dataset Before normalization', y=1.08)\n",
    "iris_df_2= pd.DataFrame(data= np.c_[normalized_x_train, y_train],\n",
    "                        columns= iris['feature_names'] + ['target'])\n",
    "di= {0.0: 'Setosa', 1.0: 'Versicolor', 2.0: 'Virginica'}\n",
    "after= sns.pairplot(iris_df_2.replace({'target':di}), hue= 'target')\n",
    "after.fig.suptitle('Pair Plot of the dataset After normalization', y=1.08)\n",
    "\n",
    "\n",
    "def distance_ecu(x_train, x_test_point):\n",
    "  distances= []  ## create empty list called distances\n",
    "  print(\"length of x_train is: \", len(x_train))\n",
    "  for row in range(len(x_train)): ## Loop over the rows of x_train\n",
    "\n",
    "      print(\"row number: \",row )\n",
    "      current_train_point= x_train[row] #Get them point by point\n",
    "      print(\"current train point is: \\n\", current_train_point)\n",
    "      print(\"current test point is: \\n\", x_test_point)\n",
    "      current_distance= 0 ## initialize the distance by zero\n",
    "\n",
    "      for col in range(len(current_train_point)): ## Loop over the columns of the row\n",
    "          current_distance += (current_train_point[col] - x_test_point[col]) **2\n",
    "          ## Or current_distance = current_distance + (x_train[i] - x_test_point[i])**2\n",
    "      current_distance= np.sqrt(current_distance) \n",
    "      print(\"current distance is: \", current_distance)\n",
    "\n",
    "      distances.append(current_distance) ## Append the distances\n",
    "      print(\"distance is: \", distances)\n",
    "\n",
    "  # Store distances in a dataframe\n",
    "  distances= pd.DataFrame(data=distances,columns=['dist'])\n",
    "  print(\"shape of distances is: \", distances.shape)\n",
    "  return distances\n",
    "\n",
    "def nearest_neighbors(distance_point, K):\n",
    "    print(\"value of k is: \", K)\n",
    "\n",
    "    # Sort values using the sort_values function\n",
    "    df_nearest= distance_point.sort_values(by=['dist'], axis=0)\n",
    "    print(\"df_nearest at axis 0\", df_nearest)\n",
    "\n",
    "    ## Take only the first K neighbors\n",
    "    df_nearest= df_nearest[:K]\n",
    "    print(\"df_nearest[:k]\", df_nearest)\n",
    "    return df_nearest\n",
    "def voting(df_nearest, y_train):\n",
    "    counter_vote= Counter(y_train[df_nearest.index])\n",
    "    print(\"counter_vote value is: \", counter_vote)\n",
    "\n",
    "    y_pred= counter_vote.most_common()[0][0]   # Majority Voting\n",
    "\n",
    "    return y_pred\n",
    "def KNN_from_scratch(x_train, y_train, x_test, K):\n",
    "    y_pred=[] \n",
    "    ## Loop over all the test set and perform the three steps\n",
    "    for x_test_point in x_test:\n",
    "      distance_point  = distance_ecu(x_train, x_test_point)  ## Step 1\n",
    "      df_nearest_point= nearest_neighbors(distance_point, K)  ## Step 2\n",
    "      y_pred_point    = voting(df_nearest_point, y_train) ## Step 3\n",
    "      y_pred.append(y_pred_point)\n",
    "\n",
    "    return y_pred\n",
    "K=3\n",
    "y_pred_scratch= KNN_from_scratch(normalized_x_train, y_train, normalized_x_test, K)\n",
    "print(y_pred_scratch)\n",
    " \n",
    "knn=KNeighborsClassifier(K)\n",
    "knn.fit(normalized_x_train, y_train)\n",
    "y_pred_sklearn= knn.predict(normalized_x_test)\n",
    "print(y_pred_sklearn)\n",
    "\n",
    "print(np.array_equal(y_pred_sklearn, y_pred_scratch))\n",
    "print(f'The accuracy of our implementation is {accuracy_score(y_test, y_pred_scratch)}')\n",
    "print(f'The accuracy of sklearn implementation is {accuracy_score(y_test, y_pred_sklearn)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EXPERIMENT 6: ID3 Algorithm\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "train_data_m = pd.read_csv(\"C:/Users/craxs/Downloads/PlayTennis.csv\") #importing the dataset from the disk\n",
    "\n",
    "print(train_data_m) #viewing some row of the dataset\n",
    "\n",
    "print(\"shape of train_data_m\", train_data_m.shape)\n",
    "print(\"number of columns of dataset are: \", train_data_m.shape[1])\n",
    "\n",
    "attributes=train_data_m.shape[1]-1\n",
    "print(\"attributes are: \", attributes)\n",
    "target=1\n",
    "print(\" target are: \", target)\n",
    "\n",
    "attributes_val=train_data_m.iloc[:,0:4]\n",
    "print(\"attributes_val \\n\", attributes_val)\n",
    "print(\"shape of attributes_val\", attributes_val.shape)\n",
    "\n",
    "target_val=train_data_m.iloc[:,-1]\n",
    "print(\"target_val \\n\", target_val)\n",
    "\n",
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "total_yes_target=0\n",
    "total_no_target=0\n",
    "for i in range (0, len(target_val)):\n",
    "  print(target_val[i])\n",
    "  if target_val[i]=='Yes':\n",
    "    total_yes_target=total_yes_target+1\n",
    "  else:\n",
    "    total_no_target=total_no_target+1\n",
    "print(\"total yes target are: \", total_yes_target)\n",
    "print(\"total no target are: \", total_no_target)\n",
    "part_a1=(total_yes_target/(total_yes_target+total_no_target))\n",
    "print(\"part_a1\", part_a1)\n",
    "if (part_a1!=0): \n",
    "  part_a=part_a1*(math.log(part_a1,2))\n",
    "  part_b1=(total_no_target/(total_yes_target+total_no_target))\n",
    "  part_b=part_b1*(math.log(part_b1,2))\n",
    "  entropy_total=-part_a-part_b\n",
    "else:\n",
    "  entropy_total=0\n",
    "print(\"Total entropy\", entropy_total)\n",
    "gain=entropy_total\n",
    "\n",
    "total_gain_columns=[]\n",
    "i=0\n",
    "for i in range (0, attributes): #attributes\n",
    "    gain=entropy_total\n",
    "    entropy=[]\n",
    "    attribute_current=train_data_m.iloc[:,i]\n",
    "\n",
    "    print(attribute_current)\n",
    "    unique_val=np.unique(attribute_current)\n",
    "    print(\"unique_val: \", unique_val)\n",
    "    for j in range (0, len(unique_val)):\n",
    "        current_unique_val=unique_val[j]\n",
    "    counts= Counter(attribute_current)\n",
    "    print(counts)\n",
    "    for c in counts.keys():\n",
    "        print(c, \":\", counts[c])\n",
    "        res_list = [k for k, value in enumerate(attribute_current) if value == c]\n",
    " \n",
    "        print(\"Count of\", c, \"is\", counts[c])\n",
    "        print(\"Index of\", c, \"is\", res_list)\n",
    "        target_new=[]\n",
    "        counter_yes=0\n",
    "        counter_no=0\n",
    "        for r in range (0, len(res_list)):\n",
    "          r1=res_list[r]\n",
    "          #print(\"r1\", r1)\n",
    "          target_current=target_val.iloc[r1]\n",
    "          if target_current=='Yes':\n",
    "            counter_yes=counter_yes+1\n",
    "          else:\n",
    "            counter_no=counter_no+1   \n",
    "\n",
    "\n",
    "          #print(\"target_current\", target_current)\n",
    "          target_new.append(target_current)\n",
    "          counts= Counter(target_new)\n",
    "          print(counts)\n",
    "        print(\"target new\",target_new)\n",
    "        print(\"Total yes:\", counter_yes)\n",
    "        print(\"Total no:\", counter_no)\n",
    "        if counter_yes==counter_no:\n",
    "          if (counter_yes==0) or (counter_no==0):\n",
    "            entropy_current=0\n",
    "          else:\n",
    "            entropy_current=1\n",
    "        elif (counter_yes==0) or (counter_no==0):\n",
    "          entropy_current=0\n",
    "        else:\n",
    "          part_a1=(counter_yes/(counter_yes+counter_no))\n",
    "          print(\"part_a1\", part_a1)\n",
    "          part_a=part_a1*(math.log(part_a1,2))\n",
    "\n",
    "          part_b1=(counter_no/(counter_yes+counter_no))\n",
    "          part_b=part_b1*(math.log(part_b1,2))\n",
    "          entropy_current=-part_a-part_b\n",
    "          print(\"entropy of\", c,\" attribute is: \", entropy_current)\n",
    "        entropy.append(entropy_current)\n",
    "     \n",
    "        print(\"Total entropy\", entropy)\n",
    "        print(len(res_list))\n",
    "        print(len(target_val))\n",
    "        current_gain=(len(res_list)/len(target_val))*entropy_current\n",
    "        print(gain)\n",
    "        print(\"current_gain\", current_gain)\n",
    "        gain=gain-current_gain\n",
    "        print(\" Gain of attribute\", c ,\"is: \", gain)\n",
    "    total_gain_columns.append(gain)\n",
    "print(\"Total gain: \", total_gain_columns)\n",
    "\n",
    "\n",
    "max_gain=max(total_gain_columns)\n",
    "index_max=total_gain_columns.index(max(total_gain_columns))\n",
    "print(\"maximum gain is \", max_gain, \"of *\", index_max, \"* attribute\")\n",
    "\n",
    "# importing the pandas library\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"C:/Users/craxs/Downloads/playTennis.csv\")\n",
    "list_of_column_names = list(df.columns)\n",
    "print('List of column names : ',list_of_column_names)\n",
    "\n",
    "attribute_name_max=list_of_column_names[index_max]\n",
    "print(attribute_name_max)\n",
    "\n",
    "attribute_max=train_data_m.iloc[:,index_max]\n",
    "print(attribute_max)\n",
    "print(target_val)\n",
    "\n",
    "unique_val=np.unique(attribute_max)\n",
    "print(unique_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EXPERIMENT 7: Naive Bayes Classifier\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "data=pd.read_csv(\"C:/Users/craxs/Downloads/PlayTennis (1).csv\")\n",
    "\n",
    "#create_array\n",
    "data_new=np.array(data)\n",
    "print(\"DATA in array format is: \\n\", data_new)\n",
    "print(\"Shape of data array is: \\n\", data_new.shape)\n",
    "\n",
    "train_data=data_new[:-1,:]\n",
    "print(train_data.shape)\n",
    "\n",
    "test_data=data_new[-1]\n",
    "print(\"Test data is: \", test_data)\n",
    "\n",
    "\n",
    "total_yes=sum(train_data[:,-1]==\"Yes\")\n",
    "total_no=sum(train_data[:,-1]==\"No\")\n",
    "print(\"Total Yes are: \", total_yes)\n",
    "print(\"Total No are: \", total_no)\n",
    "\n",
    "a1=0\n",
    "b1=0\n",
    "c1=0\n",
    "d1=0\n",
    "a0=0\n",
    "b0=0\n",
    "c0=0\n",
    "d0=0\n",
    "p_yes=total_yes/(total_yes+total_no)\n",
    "p_no=total_no/(total_yes+total_no)\n",
    "\n",
    "for i in range(len(train_data)):\n",
    "  if (train_data[i,-1]==\"Yes\"):\n",
    "    if (train_data[i,0]==test_data[0]):\n",
    "      a1=a1+1\n",
    "    if (train_data[i,1]==test_data[1]):\n",
    "      b1=b1+1\n",
    "    if (train_data[i,2]==test_data[2]):\n",
    "      c1=c1+1\n",
    "    if (train_data[i,3]==test_data[3]):\n",
    "      d1=d1+1\n",
    "\n",
    "  else:\n",
    "    if (train_data[i,0]==test_data[0]):\n",
    "      a0=a0+1\n",
    "    if (train_data[i,1]==test_data[1]):\n",
    "      b0=b0+1\n",
    "    if (train_data[i,2]==test_data[2]):\n",
    "      c0=c0+1\n",
    "    if (train_data[i,3]==test_data[3]):\n",
    "      d0=d0+1\n",
    "\n",
    "print(\"a1:\", a1)\n",
    "print(\"b1:\", b1)\n",
    "print(\"c1:\", c1)\n",
    "print(\"d1:\", d1)\n",
    "print(\"a0:\", a0)\n",
    "print(\"b0:\", b0)\n",
    "print(\"c0:\", c0)\n",
    "print(\"d0:\", d0)\n",
    "\n",
    "a1_yes=a1/total_yes\n",
    "a0_no=a0/total_no\n",
    "b1_yes=b1/total_yes\n",
    "b0_no=b0/total_no\n",
    "c1_yes=c1/total_yes\n",
    "c0_no=c0/total_no\n",
    "d1_yes=d1/total_yes\n",
    "d0_no=d0/total_no\n",
    "pyes=a1_yes*b1_yes*c1_yes*d1_yes*p_yes\n",
    "pno=a0_no*b0_no*c0_no*d0_no*p_no\n",
    "print(\"P(Yes): \", pyes)\n",
    "print(\"P(No): \", pno)\n",
    "final_prob_yes=pyes*100/(pyes+pno)\n",
    "final_prob_no=pno*100/(pyes+pno)\n",
    "print(\"Percentage of *Yes*: \", final_prob_yes)\n",
    "print(\"Percentage of *No*: \", final_prob_no)\n",
    "if (final_prob_yes>final_prob_no):\n",
    "  print(\"Answer: As 'P(Yes)>P(No)': NaN will be replaced with 'Yes' \")\n",
    "else:\n",
    "  print(\"Answer: As 'P(No)>P(Yes)': NaN will be replaced with 'No' \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EXPERIMENT 8: Naive Bayes Document\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Document Classification using Naive Bayes Classifier\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Data Reading\n",
    "d=pd.read_csv(\"C:/Users/craxs/Downloads/NBdocument.csv\")\n",
    "a=np.array(d)\n",
    "ab=a[:,1:]\n",
    "a1=[]; b1=[]; a2=[]; a3=[];a4=[]; a11=[]; a21=[]; a31=[]; a41=[];b11=[]\n",
    "b=ab\n",
    "\n",
    "# Computaion of Prior Probability\n",
    "ay=sum(b[:,-1]==\"Electrical Engineering\")/ len(b)\n",
    "an=sum(b[:,-1]==\"Computer Science & Engineering\")/len(b)\n",
    "\n",
    "# Computation of Unique Keywords\n",
    "val,col = np.unique(b,return_counts=True)\n",
    "\n",
    "# Total no. of keywords in whole document\n",
    "x=len(val)\n",
    "print('keywords in whole document are=',val,'x=',x)\n",
    "#Test data\n",
    "print('Test data i.e. any row from dataset')\n",
    "O=ab[4]\n",
    "\n",
    "# Steps for NB Classifier\n",
    "for i in range(len(b)):\n",
    "    if (b[i,-1]==\"Electrical Engineering\"):\n",
    "\n",
    "        # Total no. of words in EE Category\n",
    "        b1.append(len(b[i,:-1])); f12=np.sum(b1); #print('f12=',f12)\n",
    "\n",
    "        # Computation of Conditional Probability for EE category\n",
    "        a1.append(b[i,:]== O[0]); f1=(1+np.sum(a1))/(f12+x)\n",
    "        a2.append(b[i,:]== O[1]); f2=(1+np.sum(a2))/(f12+x)\n",
    "        a3.append(b[i,:]== O[2]); f3=(1+np.sum(a3))/(f12+x)\n",
    "        a4.append(b[i,:]== O[3]); f4=(1+np.sum(a4))/(f12+x)\n",
    "\n",
    "    else:\n",
    "         # Total no. of words in CSE Category\n",
    "        b11.append(len(b[i,:-1])); f121=np.sum(b11); #print('f121=',f121)\n",
    "\n",
    "        # Computation of Conditional Probability for CSE category\n",
    "        a11.append(b[i,:]== O[0]); f11=(1+np.sum(a11))/(f121+x)\n",
    "        a21.append(b[i,:]== O[1]); f21=(1+np.sum(a21))/(f121+x)\n",
    "        a31.append(b[i,:]== O[2]); f31=(1+np.sum(a31))/(f121+x)\n",
    "        a41.append(b[i,:]== O[3]); f41=(1+np.sum(a41))/(f121+x)\n",
    "\n",
    "# Posterior Probaility for EE category\n",
    "p_ee = ay*f1*f2*f3*f4\n",
    "\n",
    "# Posterior Probaility for CSE category\n",
    "p_cse=an*f11*f21*f31*f41\n",
    "     \n",
    "# Final outcome/ Prediction for test document\n",
    "print('\\n p_ee=',p_ee, '\\n P_cse=',p_cse)\n",
    "if(p_ee>p_cse):\n",
    "    print('\\n Document belongs to EE')\n",
    "else:\n",
    "    print('\\n Document belongs to CSE')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EXPERIMENT 9: BAYESIAN BELIEF NETWORK\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#from pgmpy.models import BayesianModel\n",
    "#from pgmpy.factors.discrete import TabularCPD\n",
    "#from pgmpy.inference import VariableElimination\n",
    "\n",
    "#Define a Structure with nodes and edge\n",
    "cancer_model = BayesianModel([('Pollution', 'Cancer'),\n",
    " ('Smoker', 'Cancer'),\n",
    " ('Cancer', 'Xray'),\n",
    " ('Cancer', 'Dyspnoea')])\n",
    "\n",
    "print('Baysian network nodes are:')\n",
    "print('\\t',cancer_model.nodes())\n",
    "print('Baysian network edges are:')\n",
    "print('\\t',cancer_model.edges())\n",
    "########\n",
    "#Creation of Conditional Probability Table\n",
    "\n",
    "cpd_poll = TabularCPD(variable='Pollution',\n",
    "                      variable_card=2,\n",
    "                      values=[[0.9], [0.1]])\n",
    "\n",
    "cpd_smoke= TabularCPD(variable='Smoker',\n",
    "                      variable_card=2,\n",
    "                      values=[[0.3], [0.7]])\n",
    "\n",
    "cpd_cancer= TabularCPD(variable='Cancer',\n",
    "                       variable_card=2,\n",
    "                       values=[[0.03, 0.05, 0.001, 0.02],\n",
    "                               [0.97, 0.95, 0.999, 0.98]],\n",
    "                       evidence=['Smoker', 'Pollution'],\n",
    "                       evidence_card=[2, 2])\n",
    "\n",
    "\n",
    "cpd_xray = TabularCPD(variable='Xray',\n",
    "                      variable_card=2,\n",
    "                      values=[[0.9, 0.2], [0.1, 0.8]],\n",
    "                      evidence=['Cancer'],\n",
    "                      evidence_card=[2])\n",
    "\n",
    "cpd_dysp = TabularCPD(variable='Dyspnoea',\n",
    "                      variable_card=2,\n",
    "                      values=[[0.65, 0.3], [0.35, 0.7]],\n",
    "                      evidence=['Cancer'],\n",
    "                      evidence_card=[2])\n",
    "#######\n",
    "# Associating the parameters with the model structure.\n",
    "cancer_model.add_cpds(cpd_poll, cpd_smoke, cpd_cancer, cpd_xray, cpd_dysp)\n",
    "print('Model generated by adding conditional probability disttributions(cpds)')\n",
    "\n",
    "# Checking if the cpds are valid for the model.\n",
    "print('Checking for Correctness of model : ', end='' )\n",
    "print(cancer_model.check_model())\n",
    "\n",
    "'''print('All local idependencies are as follows')\n",
    "cancer_model.get_independencies()\n",
    "'''\n",
    "print('Displaying CPDs')\n",
    "print(cancer_model.get_cpds('Pollution'))\n",
    "print(cancer_model.get_cpds('Smoker'))\n",
    "print(cancer_model.get_cpds('Cancer'))\n",
    "print(cancer_model.get_cpds('Xray'))\n",
    "print(cancer_model.get_cpds('Dyspnoea'))\n",
    "######\n",
    "# Check for d-separation between variables\n",
    "print(cancer_model.is_dconnected(\"Pollution\", \"Smoker\"))\n",
    "print(cancer_model.is_dconnected(\"Pollution\", \"Smoker\", observed=[\"Cancer\"]))\n",
    "########\n",
    "# Computing the probability of Cancer given smoke.\n",
    "cancer_infer = VariableElimination(cancer_model)\n",
    "print('\\nInferencing with Bayesian Network');\n",
    "print('\\nProbability of Cancer given Smoker')\n",
    "q = cancer_infer.query(variables=['Cancer'], evidence={'Smoker': 1})\n",
    "print(q)\n",
    "\n",
    "q = cancer_infer.query(variables=['Cancer'], evidence={'Smoker': 0})\n",
    "print(q)\n",
    "\n",
    "print('\\nProbability of Cancer given Smoker,Pollution')\n",
    "q = cancer_infer.query(variables=['Cancer'], evidence={'Smoker': 1,'Pollution': 1})\n",
    "print(q)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EXPERIMENT 10: K-Means Clustering\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import random as rd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "dataset=pd.read_csv(\"C:/Users/craxs/Downloads/Book1.csv\")\n",
    "dataset.describe()\n",
    "X = dataset.iloc[:, [1, 2]].values\n",
    "print(dataset.describe())\n",
    "print(dataset.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p=X.shape[0] #number of training examples\n",
    "g=X.shape[1] #number of features. Here n=2\n",
    "print(\" p x g is: \", p, 'x', g)\n",
    "g_iter=100\n",
    "K=5 # number of clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Centroids=np.array([]).reshape(g,0)\n",
    "print(Centroids)\n",
    "\n",
    "for i in range(K):\n",
    "    rand=rd.randint(0,p-1)\n",
    "    print(\"rand\",rand)\n",
    "    print(\"X[rand]\", X[rand])\n",
    "    Centroids=np.c_[Centroids,X[rand]]\n",
    "print(\"Centroid of clusters\", Centroids)\n",
    "Output={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EuclidianDistance=np.array([]).reshape(p,0)\n",
    "for k in range(K):\n",
    "       tempDist=np.sum((X-Centroids[:,k])**2,axis=1)\n",
    "       EuclidianDistance=np.c_[EuclidianDistance,tempDist]\n",
    "print(EuclidianDistance.shape)\n",
    "C=np.argmin(EuclidianDistance,axis=1)+1\n",
    "#print(\"cluster of each sample is: \", C)\n",
    "print(\"shape of C is: \", C.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y={}\n",
    "for k in range(K):\n",
    "    Y[k+1]=np.array([]).reshape(2,0)\n",
    "for i in range(p):\n",
    "    Y[C[i]]=np.c_[Y[C[i]],X[i]]\n",
    "for k in range(K):\n",
    "    Y[k+1]=Y[k+1].T\n",
    "for k in range(K):\n",
    "     Centroids[:,k]=np.mean(Y[k+1],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(g_iter):\n",
    "     #step 2.a\n",
    "      EuclidianDistance=np.array([]).reshape(p,0)\n",
    "      for k in range(K):\n",
    "          tempDist=np.sum((X-Centroids[:,k])**2,axis=1)\n",
    "          EuclidianDistance=np.c_[EuclidianDistance,tempDist]\n",
    "      C=np.argmin(EuclidianDistance,axis=1)+1\n",
    "     #step 2.b\n",
    "      Y={}\n",
    "      for k in range(K):\n",
    "          Y[k+1]=np.array([]).reshape(2,0)\n",
    "      for i in range(p):\n",
    "          Y[C[i]]=np.c_[Y[C[i]],X[i]]\n",
    "\n",
    "      for k in range(K):\n",
    "          Y[k+1]=Y[k+1].T\n",
    "\n",
    "      for k in range(K):\n",
    "          Centroids[:,k]=np.mean(Y[k+1],axis=0)\n",
    "      Output=Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:,0],X[:,1],c='black',label='unclustered data')\n",
    "plt.xlabel('Income')\n",
    "plt.ylabel('Number of transactions')\n",
    "plt.legend()\n",
    "plt.title('Plot of data points')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color=['red','blue','green','cyan','magenta']\n",
    "labels=['cluster1','cluster2','cluster3','cluster4','cluster5']\n",
    "for k in range(K):\n",
    "    print(color[k])\n",
    "    plt.scatter(Output[k+1][:,0],Output[k+1][:,1],color=color[k],label=labels[k])\n",
    "plt.scatter(Centroids[0,:],Centroids[1,:],s=300,c='yellow',label='Centroids')\n",
    "plt.xlabel('Income')\n",
    "plt.ylabel('Number of transactions')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
